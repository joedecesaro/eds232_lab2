---
title: "Lab 2 - Community"
author: "Joe DeCesaro"
date: "1/29/2022"
output: html_document
bibliography: ["ml-env.bib"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lab 2a

## Learning Objectives {.unnumbered}

In this lab, you will play with **unsupervised classification** techniques while working with **ecological community** datasets.

- Comparing species counts between sites using **distance** metrics:

  - **Euclidean** calculates the distance between a virtualized space using Pythagorean theorem.
  
  - **Manhattan** calculates integer "around the block" difference.
  
  - **Bray-Curtis** dissimilarity is based on the sum of lowest counts of shared species between sites over the sum of all species. A dissimilarity value of 1 is completely dissimilar, i.e. no species shared. A value of 0 is completely identical.

- **Clustering**

  - **_K_-Means clustering** with function `kmeans()` given a pre-assigned number of clusters assigns membership centroid based on reducing within cluster variation.
  
    - **Voronoi diagrams** visualizes regions to nearest points, useful here to show membership of nodes to nearest centroid.
  
  - **Hierarchical clustering** allows for a non-specific number of clusters. 
  
    - **Agglomerative hierarchical clustering**, such as with `diana()`, agglomerates as it builds the tree. It is good at identifying small clusters.

    - **Divisive hierarchical clustering**, such as with `agnes()`, divides as it builds the tree. It is good at identifying large clusters.
    
    - **Dendrograms** visualize the branching tree.

- **Ordination** (coming Monday)

## Clustering

**Clustering** associates similar data points with each other, adding a grouping label. It is a form of **unsupervised learning** since we don't fit the model based on feeding it a labeled response (i.e. $y$). 

### _K_-Means Clustering

Source: [K Means Clustering in R | DataScience+](https://datascienceplus.com/k-means-clustering-in-r/)

In _k_-means clustering, the number of clusters needs to be specified. The algorithm randomly assigns each observation to a cluster, and finds the centroid of each cluster. Then, the algorithm iterates through two steps:

1. Reassign data points to the cluster whose centroid is closest.
1. Calculate new centroid of each cluster.

These two steps are repeated until the within cluster variation cannot be reduced any further. The within cluster variation is calculated as the sum of the euclidean distance between the data points and their respective cluster centroids.

#### Load and plot the `iris` dataset
```{r}
# load R packages
librarian::shelf(
  dplyr, DT, ggplot2, tibble)

# set seed for reproducible results
set.seed(42)

# load the dataset
data("iris")

# look at documentation in RStudio
if (interactive())
  help(iris)

# show data table
datatable(iris)

# plot petal length vs width, species naive
ggplot(
  iris, aes(Petal.Length, Petal.Width)) +
  geom_point()

# plot petal length vs width, color by species
legend_pos <- theme(
    legend.position = c(0.95, 0.05),
    legend.justification = c("right", "bottom"),
    legend.box.just = "right")
ggplot(
  iris, aes(Petal.Length, Petal.Width, color = Species)) +
  geom_point() +
  legend_pos
```

#### Cluster `iris` using `kmeans()`

```{r}
# cluster using kmeans
k <- 3  # number of clusters
iris_k <- kmeans(
  iris %>% 
    select(Petal.Length, Petal.Width), 
  centers = k)

# show cluster result
iris_k

# compare clusters with species (which were not used to cluster)
table(iris_k$cluster, iris$Species)
```

**Question**: How many observations could be considered "misclassified" if expecting petal length and width to differentiate between species?
6 observations.

```{r}
# extract cluster assignment per observation
Cluster = factor(iris_k$cluster)

ggplot(iris, aes(Petal.Length, Petal.Width, color = Cluster)) +
  geom_point() + 
  legend_pos
```

```{r, eval=F, echo=F}
# **Task**: Highlight the "misclassified" points in the plot. _Hints: To get just the points misclassified, you can use `iris_k$cluster != as.integer(iris$Species)`, which can feed as the second argument into `filter(iris)`. To add another set of points to the ggplot, use `+ geom_point()` with arguments for: `data` with the additional points, `pch` [point shape](https://www.r-bloggers.com/2021/06/r-plot-pch-symbols-different-point-shapes-in-r/) with `fill=NA` for transparency and outline `color="red"`._
obs_mis <- iris %>% 
  filter(iris_k$cluster != as.integer(iris$Species))
ggplot(iris, aes(Petal.Length, Petal.Width, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_point(data = obs_mis, color="red", fill=NA, pch=21)
```

#### Plot Voronoi diagram of clustered `iris`

This form of clustering assigns points to the cluster based on nearest centroid. You can see the breaks more clearly with a [Voronoi diagram](https://en.wikipedia.org/wiki/Voronoi_diagram).

```{r}
librarian::shelf(ggvoronoi, scales)

# define bounding box for geom_voronoi()
box <- tribble(
  ~Petal.Length, ~Petal.Width, ~group,
  1, 0.1, 1,
  1, 2.5, 1,
  7, 2.5, 1,
  7, 0.1, 1,
  1, 0.1, 1) %>% 
  data.frame()

# cluster using kmeans
k <- 3  # number of clusters
iris_k <- kmeans(
  iris %>% 
    select(Petal.Length, Petal.Width), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(iris_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(iris_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(iris, aes(Petal.Length, Petal.Width, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, outline = box) + 
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")
```

**Task**: Show the Voronoi diagram for fewer (`k=2`) and more (`k=8`) clusters to see how assignment to cluster centroids work.
```{r}
# cluster using kmeans
k <- 2  # number of clusters
iris_k <- kmeans(
  iris %>% 
    select(Petal.Length, Petal.Width), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(iris_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(iris_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(iris, aes(Petal.Length, Petal.Width, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, outline = box) + 
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")

# cluster using kmeans
k <- 8  # number of clusters
iris_k <- kmeans(
  iris %>% 
    select(Petal.Length, Petal.Width), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(iris_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(iris_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(iris, aes(Petal.Length, Petal.Width, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, outline = box) + 
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")
```
### Hierarchical Clustering

Next, you'll cluster sites according to species composition. You'll use the `dune` dataset from the `vegan` R package.

#### Load `dune` dataset

```{r}
librarian::shelf(
  cluster, vegan)

# load dune dataset from package vegan
data("dune")

# show documentation on dataset if interactive
if (interactive())
  help(dune)
```

**Question**: What are the rows and columns composed of in the `dune` data frame?


