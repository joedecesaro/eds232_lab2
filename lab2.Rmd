---
title: "Lab 2 - Community"
author: "Joe DeCesaro"
date: "1/29/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lab 2a - Cluster

## Learning Objectives {.unnumbered}

In this lab, you will play with **unsupervised classification** techniques while working with **ecological community** datasets.

- Comparing species counts between sites using **distance** metrics:

  - **Euclidean** calculates the distance between a virtualized space using Pythagorean theorem.
  
  - **Manhattan** calculates integer "around the block" difference.
  
  - **Bray-Curtis** dissimilarity is based on the sum of lowest counts of shared species between sites over the sum of all species. A dissimilarity value of 1 is completely dissimilar, i.e. no species shared. A value of 0 is completely identical.

- **Clustering**

  - **_K_-Means clustering** with function `kmeans()` given a pre-assigned number of clusters assigns membership centroid based on reducing within cluster variation.
  
    - **Voronoi diagrams** visualizes regions to nearest points, useful here to show membership of nodes to nearest centroid.
  
  - **Hierarchical clustering** allows for a non-specific number of clusters. 
  
    - **Agglomerative hierarchical clustering**, such as with `diana()`, agglomerates as it builds the tree. It is good at identifying small clusters.

    - **Divisive hierarchical clustering**, such as with `agnes()`, divides as it builds the tree. It is good at identifying large clusters.
    
    - **Dendrograms** visualize the branching tree.

- **Ordination** (coming Monday)

## Clustering

**Clustering** associates similar data points with each other, adding a grouping label. It is a form of **unsupervised learning** since we don't fit the model based on feeding it a labeled response (i.e. $y$). 

### _K_-Means Clustering

Source: [K Means Clustering in R | DataScience+](https://datascienceplus.com/k-means-clustering-in-r/)

In _k_-means clustering, the number of clusters needs to be specified. The algorithm randomly assigns each observation to a cluster, and finds the centroid of each cluster. Then, the algorithm iterates through two steps:

1. Reassign data points to the cluster whose centroid is closest.
1. Calculate new centroid of each cluster.

These two steps are repeated until the within cluster variation cannot be reduced any further. The within cluster variation is calculated as the sum of the euclidean distance between the data points and their respective cluster centroids.

#### Load and plot the `penguins` dataset
```{r}
# load R packages
librarian::shelf(
  dplyr, DT, ggplot2, palmerpenguins, skimr, tibble)

# set seed for reproducible results
set.seed(42)

# load the dataset
data("penguins")

# look at documentation in RStudio
if (interactive())
  help(penguins)

# show data table
datatable(penguins)
# skim the table for a summary
skim(penguins)

# remove the rows with NAs
penguins <- na.omit(penguins)
```

```{r}
# plot bill length vs depth, species naive
ggplot(
  penguins, aes(bill_length_mm, bill_depth_mm)) +
  geom_point()

# plot bill length vs depth, color by species
legend_pos <- theme(
    legend.position = c(0.95, 0.05),
    legend.justification = c("right", "bottom"),
    legend.box.just = "right")
ggplot(
  penguins, aes(bill_length_mm, bill_depth_mm, color = species)) +
  geom_point() +
  legend_pos
```

#### Cluster `penguins` using `kmeans()`

```{r}
# cluster using kmeans
k <- 3  # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

# show cluster result
penguins_k

# compare clusters with species (which were not used to cluster)
table(penguins_k$cluster, penguins$species)
```

```{r}
# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos
```

**Question:** Comparing the observed species plot with 3 species with the kmeans() cluster plot with 3 clusters, where does this “unsupervised” kmeans() technique (that does not use species to “fit” the model) produce similar versus different results? One or two sentences would suffice. Feel free to mention ranges of values along the axes.

- The kmeans() cluster plot seems to identify group 1 similarly with the Adelie group in the species plot. The kmeans() cluster plot seems to seperate the clusters along the bill_length measurement more than the bill_depth. In the species plot we can see that the Chinstrap and Gentoo seem to have similar bill_length measurements and differing bill_depth. 

#### Plot Voronoi diagram of clustered `penguins`

This form of clustering assigns points to the cluster based on nearest centroid. You can see the breaks more clearly with a [Voronoi diagram](https://en.wikipedia.org/wiki/Voronoi_diagram).

```{r}
librarian::shelf(ggvoronoi, scales)

# define bounding box for geom_voronoi()
xr <- extendrange(range(penguins$bill_length_mm), f=0.1)
yr <- extendrange(range(penguins$bill_depth_mm), f=0.1)
box <- tribble(
  ~bill_length_mm, ~bill_depth_mm, ~group,
  xr[1], yr[1], 1,
  xr[1], yr[2], 1,
  xr[2], yr[2], 1,
  xr[2], yr[1], 1,
  xr[1], yr[1], 1) %>% 
  data.frame()

# cluster using kmeans
k <- 3  # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(penguins_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, 
    outline = box) + 
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")
```

**Task**: Show the Voronoi diagram for fewer (`k=2`) and more (`k=8`) clusters to see how assignment to cluster centroids work.

```{r}
# cluster using kmeans
k <- 2  # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(penguins_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, 
    outline = box) + 
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")

# cluster using kmeans
k <- 8  # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(penguins_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, 
    outline = box) + 
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")
```
### Hierarchical Clustering

Next, you'll cluster sites according to species composition. You'll use the `dune` dataset from the `vegan` R package.

#### Load `dune` dataset

```{r}
librarian::shelf(
  cluster, vegan)

# load dune dataset from package vegan
data("dune")

# show documentation on dataset if interactive
if (interactive())
  help(dune)
```

**Question**: What are the rows and columns composed of in the `dune` data frame?

- Rows: the 20 different sites
- Columns: the 30 different species

#### Calculate Ecological Distances on `sites`

Before we calculate ecological distance between sites for `dune`, let's look at these metrics with a simpler dataset, like the example given in Chapter 8 by @kindtTreeDiversityAnalysis2005.

```{r}
sites <- tribble(
  ~site, ~sp1, ~sp2, ~sp3,
    "A",    1,    1,    0,
    "B",    5,    5,    0,
    "C",    0,    0,    1) %>% 
  column_to_rownames("site")
sites

sites_manhattan <- vegdist(sites, method="manhattan")
sites_manhattan

sites_euclidean <- vegdist(sites, method="euclidean")
sites_euclidean

sites_bray <- vegdist(sites, method="bray")
sites_bray
```
**Question:** In your own words, how does Bray Curtis differ from Euclidean distance? See sites_euclidean versus sites_bray from lab code, slides from Lecture 05. Clustering and reading Chapter 8 of Kindt and Coe (2005).

- Euclidean Distance: measures the distance between 2 points and depends on the abundance of each species, not just species shared. Can technically have any value greater than zero if not standardized with proportionality.
- Bray Curtis: dissimilarity is based on the sum of lowest counts of shared species between sites over the sum of all species. Only has values range from 0 to 1.


#### Bray-Curtis Dissimilarity on `sites` 

Let's take a closer look at the [Bray-Curtis Dissimilarity](https://en.wikipedia.org/wiki/Bray%E2%80%93Curtis_dissimilarity) distance:

So to calculate Bray-Curtis for the example `sites`: 

- $B_{AB} = 1 - \frac{2 * (1 + 1)}{2 + 10} = 1 - 4/12 = 1 - 1/3 = 0.667$

- $B_{AC} = 1 - \frac{2 * 0}{2 + 1} = 1$

- $B_{BC} = 1 - \frac{2 * 0}{10 + 1} = 1$

#### Agglomerative hierarchical clustering on `dune` 

See text to accompany code: _HOMLR_ [21.3.1 Agglomerative hierarchical clustering](https://bradleyboehmke.github.io/HOML/hierarchical.html#agglomerative-hierarchical-clustering).

```{r}
# Dissimilarity matrix
d <- vegdist(dune, method="bray")
dim(d)
as.matrix(d)[1:5, 1:5]

# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )
```


```{r}
# Dendrogram plot of hc1
plot(hc1, cex = 0.6, hang = -1)

# Compute agglomerative clustering with agnes
hc2 <- agnes(dune, method = "complete")

# Agglomerative coefficient for complete linkage
hc2$ac

# Dendrogram plot of hc2
plot(hc2, which.plot = 2)

# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(dune, method = x)$ac
}

# get agglomerative coefficient for each linkage method
purrr::map_dbl(m, ac)

# Compute ward linkage clustering with agnes
hc3 <- agnes(dune, method = "ward")

# Agglomerative coefficient
hc3$ac

# Dendrogram plot of hc3
plot(hc3, which.plot = 2)
```
**Question:** Which function comes first, vegdist() or hclust(), and why? See HOMLR 21.3.1 Agglomerative hierarchical clustering.

- vegdist() comes first because you need to generate a matrix with a specified distance method for the hclust() function to use to when it builds its clusters.

**Question:** In your own words, how does hclust() differ from agnes()? See HOMLR 21.3.1 Agglomerative hierarchical clustering and help documentation (?hclust(), ?agnes()).

- agnes() generally does the same things as hclust() with the addition that it also can give you the agglomerative coefficient, which is a measure of the strength of the clustering structure. Values closer to 1 suggest a more balanced cluster structure.

**Question:** Of the 4 methods, which is the “best” model in terms of Agglomerative Coefficient?

- The “ward” method has the highest AC and would therefore be considered the “best”

#### Divisive hierarchical clustering on `dune` 

See text to accompany code: _HOMLR_ [21.3.2 Divisive hierarchical clustering](https://bradleyboehmke.github.io/HOML/hierarchical.html#divisive-hierarchical-clustering).

```{r}
# compute divisive hierarchical clustering
hc4 <- diana(dune)

# Divise coefficient; amount of clustering structure found
hc4$dc
```

**Question:**In your own words, how does agnes() differ from diana()? See HOMLR21.3.1 Agglomerative hierarchical clustering, slides from Lecture 05. Clusteringand help documentation (?agnes(),  ?diana()).

- agnes() performs agglomerative hierarchical clustering (leaves to root) and computes an agglomerative coefficient. diana() performs divisive hierarchical clustering (root to leaves) and computes a divisive coefficient (closer to 1 suggests stronger group distinctions).

#### Determining Optimal Clusters
```{r}
librarian::shelf(factoextra)

# Plot cluster results
p1 <- fviz_nbclust(dune, FUN = hcut, method = "wss",  k.max = 10) +
  ggtitle("(A) Elbow method")

p2 <- fviz_nbclust(dune, FUN = hcut, method = "silhouette", k.max = 10) +
  ggtitle("(B) Silhouette method")

p3 <- fviz_nbclust(dune, FUN = hcut, method = "gap_stat", k.max = 10) +
  ggtitle("(C) Gap statistic")

# Display plots side by side
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```

**Question:** How do the optimal number of clusters compare between methods for those with a dashed line?

- For the silhouette method the optimal number of clusters is 4, for the gap statistic method the optimal number of clusters is 3

#### Working with dendrograms

See text to accompany code: _HOMLR_ [21.5 Working with dendrograms](https://bradleyboehmke.github.io/HOML/hierarchical.html#working-with-dendrograms).

```{r}
# Construct dendorgram for the Ames housing example
hc5 <- hclust(d, method = "ward.D2" )
dend_plot <- fviz_dend(hc5)
dend_data <- attr(dend_plot, "dendrogram")
dend_cuts <- cut(dend_data, h = 8)
fviz_dend(dend_cuts$lower[[2]])

# Ward's method
hc5 <- hclust(d, method = "ward.D2" )

# Cut tree into 4 groups
k = 4
sub_grp <- cutree(hc5, k = k)

# Number of members in each cluster
table(sub_grp)

# Plot full dendogram
fviz_dend(
  hc5,
  k = k,
  horiz = TRUE,
  rect = TRUE,
  rect_fill = TRUE,
  rect_border = "jco",
  k_colors = "jco")
```

**Question:** In dendrogram plots, which is the biggest determinant of relatedness between observations: the distance between observations along the labeled axes or the height of their shared connection? See HOMLR 21.5 Working with dendrograms.

- The height of their shared connection

# Lab 2b - Ordination

## Learning Objectives {.unnumbered}

In this lab, you will play with **unsupervised classification** techniques while working with **ecological community** datasets.

- **Ordination** orders sites near each other based on similarity. It is a multivariate analysis technique used to effectively collapse dependent axes into fewer dimensions, i.e. dimensionality reduction.

  - **Principal Components Analyses (PCA)** is the most common and oldest technique that assumes linear relationships between axes. You will follow a non-ecological example from [Chapter 17 Principal Components Analysis | Hands-On Machine Learning with R](https://bradleyboehmke.github.io/HOML/pca.html) to learn about this commonly used technique.
  
  - **Non-metric MultiDimensional Scaling (NMDS)** allows for non-linear relationships. This ordination technique is implemented in the R package [`vegan`](https://cran.r-project.org/web/packages/vegan/index.html). You'll use an ecological dataset, species and environment from lichen pastures that reindeer forage upon, with excerpts from the [vegantutor vignette](https://github.com/bbest/eds232-ml/raw/main/files/vegantutor.pdf) ([source](https://github.com/jarioksa/vegandocs)) to apply these techniques:
    - **Unconstrained ordination** on species using NMDS;
    - Overlay with environmental gradients; and
    - **Constrained ordination** on species and environmnent using another ordination technique, **canonical correspondence analysis (CCA)**.

## Principal Components Analysis (PCA)

Although this example uses a non-ecological dataset, it goes through the materials walk through the idea and procedure of conducting an ordination using the most widespread technique.

Please read the entirety of [Chapter 17 Principal Components Analysis | Hands-On Machine Learning with R](https://bradleyboehmke.github.io/HOML/pca.html#finding-principal-components). Supporting text is mentioned below where code is run.

### Prerequisites

See supporting text: [17.1 Prerequisites](https://bradleyboehmke.github.io/HOML/pca.html#prerequisites-14)

```{r}
# load R packages
librarian::shelf(
  dplyr, ggplot2, h2o)

# set seed for reproducible results
set.seed(42)

# get data
url <- "https://koalaverse.github.io/homlr/data/my_basket.csv"
my_basket <- readr::read_csv(url)
dim(my_basket)

my_basket
```

From [Section 1.4](https://bradleyboehmke.github.io/HOML/intro.html#data):

- `my_basket.csv`: Grocery items and quantities purchased. Each observation represents a single basket of goods that were purchased together.
  * Problem type: unsupervised basket analysis
  * response variable: NA
  * features: 42
  * observations: 2,000
  * objective: use attributes of each basket to identify common groupings of items purchased together.

### Performing PCA in R

See supporting text: [17.4 Performing PCA in R](https://bradleyboehmke.github.io/HOML/pca.html#performing-pca-in-r)

```{r}
h2o.no_progress()  # turn off progress bars for brevity
h2o.init(max_mem_size = "5g")  # connect to H2O instance

# convert data to h2o object
my_basket.h2o <- as.h2o(my_basket)

# run PCA
my_pca <- h2o.prcomp(
  training_frame = my_basket.h2o,
  pca_method = "GramSVD",
  k = ncol(my_basket.h2o), 
  transform = "STANDARDIZE", 
  impute_missing = TRUE,
  max_runtime_secs = 1000)
my_pca

my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, reorder(feature, pc1))) +
  geom_point()

my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, pc2, label = feature)) +
  geom_text()
```

**Question:** Why is the pca_method of “GramSVD” chosen over “GLRM”?

- It is recommended to use GLRM when your data contains mostly categorical variables and GramSVD when data is mostly numeric. This data is mostly numeric so GramSVD is used.

**Question:** How many initial principal components are chosen with respect to dimensions of the input data?

- There are `r ncol(my_basket.h2o)`, these are all columns of the input data.

**Question:** What category of grocery items contribute most to PC1? (These are related because they're bought most often together on a given grocery trip)

- Alcohol products, candy bars, and "quick stop" items

**Question:** What category of grocery items contribute the least to PC1 but positively towards PC2?

- Vegetables contribute the least to PC1 and drinks such as coffee, milk, and tea contribute the most to PC2

### Eigenvalue criterion

See supporting text: [17.5.1 Eigenvalue criterion](https://bradleyboehmke.github.io/HOML/pca.html#eigenvalue-criterion).

```{r}
# Compute eigenvalues
eigen <- my_pca@model$importance["Standard deviation", ] %>%
  as.vector() %>%
  .^2
  
# Sum of all eigenvalues equals number of variables
sum(eigen)
## [1] 42

# Find PCs where the sum of eigenvalues is greater than or equal to 1
which(eigen >= 1)

# Extract PVE and CVE
ve <- data.frame(
  PC  = my_pca@model$importance %>% seq_along(),
  PVE = my_pca@model$importance %>% .[2,] %>% unlist(),
  CVE = my_pca@model$importance %>% .[3,] %>% unlist())

# Plot PVE and CVE
ve %>%
  tidyr::gather(metric, variance_explained, -PC) %>%
  ggplot(aes(PC, variance_explained)) +
  geom_point() +
  facet_wrap(~ metric, ncol = 1, scales = "free")

# How many PCs required to explain at least 75% of total variability
min(which(ve$CVE >= 0.75))

# Screee plot criterion
data.frame(
  PC  = my_pca@model$importance %>% seq_along,
  PVE = my_pca@model$importance %>% .[2,] %>% unlist()) %>%
  ggplot(aes(PC, PVE, group = 1, label = PC)) +
  geom_point() +
  geom_line() +
  geom_text(nudge_y = -.002)
```

**Question:** How many principal components would you include to explain 90% of the total variance?

- About 35 PCs

**Question:** How many principal components to include up to the elbow of the PVE, i.e. the “elbow” before plateau of dimensions explaining the least variance?

- 8 PCs

**Question:** What are a couple of disadvantages to using PCA? See HOMLR 17.6 Final thoughts.

- PCA can be highly affected by outliers.
- Traditional PCA does not perform as well in very high dimensional space where complex nonlinear patterns often exist.

## Non-metric MultiDimensional Scaling (NMDS)

### Unconstrained Ordination on Species

See supporting text: **2.1 Non-metric Multidimensional scaling** in [vegantutor.pdf](https://github.com/bbest/eds232-ml/raw/main/files/vegantutor.pdf): 

```{r}
# load R packages
librarian::shelf(
  vegan, vegan3d)

# vegetation and environment in lichen pastures from Vare et al (1995)
data("varespec") # species
data("varechem") # chemistry

varespec %>% tibble()
vare.dis <- vegdist(varespec)
vare.mds0 <- monoMDS(vare.dis)
stressplot(vare.mds0)

ordiplot(vare.mds0, type = "t")

vare.mds <- metaMDS(varespec, trace = FALSE)
vare.mds

plot(vare.mds, type = "t")
```

**Question:** What are the dimensions of the varespec data frame and what do rows versus columns represent?

- Dimensions: 24 rows x 44 columns
- Rows: Lichen Pasture
- Columns: vegetation and environment

**Question:** The “stress” in a stressplot represents the difference between the observed input distance versus the fitted ordination distance. How much better is the non-metric (i.e., NMDS) fit versus a linear fit (as with PCA) in terms of \(R^2\)?

- The R^2 for the NMDS is 0.99 and the R^2 for the linear fit is 0.94, the NMDS fit is about 5% better

**Question:** What two sites are most dissimilar based on species composition for the first component MDS1? And two more most dissimilar sites for the second component MDS2?

- Sites 5 and 28 for MDS1
- Sites 14 and 21 for MDS2

**Question:** What is the basic difference between metaMDS and monoMDS()? See 2.1 Non-metric Multidimensional scaling of vegantutor.pdf.

- monoMDS is a long list of including the final configuration and the stress
- metaMDS allows you to run MDS with several random starts

### Overlay with Environment

See supporting text in [vegantutor.pdf](https://github.com/bbest/eds232-ml/raw/main/files/vegantutor.pdf): 
  * 3 Environmental interpretation
  * 3.1 Vector fitting
  * 3.2 Surface fitting

```{r}
ef <- envfit(vare.mds, varechem, permu = 999)
ef

plot(vare.mds, display = "sites")
plot(ef, p.max = 0.05)

ef <- envfit(vare.mds ~ Al + Ca, data = varechem)
plot(vare.mds, display = "sites")
plot(ef)

tmp <- with(varechem, ordisurf(vare.mds, Al, add = TRUE))
ordisurf(vare.mds ~ Ca, data=varechem, add = TRUE, col = "green4")
```

**Question:** What two soil chemistry elements have the strongest negative relationship with NMDS1 that is based on species composition?

- Al and Fe

**Question:** Which of the two NMDS axes differentiates Ca the most, i.e. has the highest value given by the contours at the end (and not middle) of the axis?

- NMDS1


### Constrained Ordination on Species and Environment

See supporting text in [vegantutor.pdf](https://github.com/bbest/eds232-ml/raw/main/files/vegantutor.pdf): 
  * 4 Constrained ordination
  * 4.1 Model specification
  
Technically, this uses another technique `cca`, or canonical correspondence analysis.

```{r}
# ordinate on species constrained by three soil elements
vare.cca <- cca(varespec ~ Al + P + K, varechem)
vare.cca

# plot ordination
plot(vare.cca)

# plot 3 dimensions
ordiplot3d(vare.cca, type = "h")
  
if (interactive()){
  ordirgl(vare.cca)
}
```

**Question:** What is the difference between “constrained” versus “unconstrained” ordination within ecological context?

- Unconstrained ordination we first find the major compositional variation, and then relate this variation to observed environmental variation.
- In constrained ordination we do not want to display all or even most of the compositional variation, but only the variation that can be explained by the used environmental variables, or constraints.

**Question:**  What sites are most differentiated by CCA1, i.e. furthest apart along its axis, based on species composition AND the environment? What is the strongest environmental vector for CCA1, i.e. longest environmental vector in the direction of the CCA1 axes?

- Sites 4 and 28
- Al
